# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JxmOEopMNoyDwti9CfQeHEpnS7AEksTv

#***Question 1***
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
df = pd.read_csv('annealtest.csv', header=None)

column_names = ['family', 'product-type', 'steel', 'carbon', 'hardness', 'temper_rolling',
                'condition', 'formability', 'strength', 'non-ageing', 'surface-finish',
                'surface-quality', 'enamelability', 'bc', 'bf', 'bt', 'bw/me', 'bl', 'm',
                'chrom', 'phos', 'cbond', 'marvi', 'exptl', 'ferro', 'corr',
                'blue/bright/varn/clean', 'lustre', 'jurofm', 's', 'p', 'shape', 'thick',
                'width', 'len', 'oil', 'bore', 'packing', 'class']

df.columns = column_names

print(df.info())
print("\nSample data:")
print(df.head())

# Check for missing values
print("\nMissing values:")
print(df.isnull().sum())

# Describe numerical columns
print("\nNumerical columns description:")
print(df.describe())

# Plot distribution of classes
plt.figure(figsize=(10, 6))
df['class'].value_counts().plot(kind='bar')
plt.title('Distribution of Classes')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

# Correlation heatmap for numerical features
numerical_features = df.select_dtypes(include=[np.number]).columns
plt.figure(figsize=(12, 10))
sns.heatmap(df[numerical_features].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer

# Handle missing values
numeric_cols = df.select_dtypes(include=np.number).columns
categorical_cols = df.select_dtypes(exclude=np.number).columns

df[numeric_cols] = df[numeric_cols].replace('?', np.nan).fillna(df[numeric_cols].mean()) # For numerical columns, replaced '?' with NaN and then filled with mean

# Check if there are any categorical columns
if len(categorical_cols) > 0:
    df[categorical_cols] = df[categorical_cols].replace('?', df[categorical_cols].mode().iloc[0]) # For categorical columns, replaced '?' with mode

# Converting categorical variables to numerical
le = LabelEncoder()
for column in df.select_dtypes(include=['object']):
    df[column] = le.fit_transform(df[column])

# Normalizing numerical inputs
scaler = StandardScaler()
numerical_columns = df.select_dtypes(include=[np.number]).columns.drop('class')

df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

print("Preprocessed data:")
print(df.head())
print("\nMissing values after preprocessing:")
print(df.isnull().sum())

"""#***Question 2***
Part 1,2,3
"""

import numpy as np

class PCA:
    def __init__(self):
        self.eigenvectors = None
        self.eigenvalues = None
        self.explained_variance = None

    def compute_covariance_matrix(self, X):
        n_samples = X.shape[0]
        centered_X = X - np.mean(X, axis=0)
        return np.dot(centered_X.T, centered_X) / (n_samples - 1)

    def fit(self, X):
        cov_matrix = self.compute_covariance_matrix(X)

        self.eigenvalues, self.eigenvectors = np.linalg.eig(cov_matrix)

        idx = self.eigenvalues.argsort()[::-1]  # Sorted eigenvectors by decreasing eigenvalues
        self.eigenvalues = self.eigenvalues[idx]
        self.eigenvectors = self.eigenvectors[:, idx]

        total_variance = np.sum(self.eigenvalues)
        self.explained_variance = self.eigenvalues / total_variance

    def transform(self, X, n_components):

        X_centered = X - np.mean(X, axis=0) # centred the data

        return np.dot(X_centered, self.eigenvectors[:, :n_components]) # Projected the centered data onto the first n_components eigenvectors

X = df.drop('class', axis=1).values
y = df['class'].values

pca = PCA()
pca.fit(X)

n_components = 2  # for example, i chose 2
X_reduced = pca.transform(X, n_components)

# Plot scatter plot with eigenvectors
plt.figure(figsize=(14, 8))
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap='viridis')
for i in range(n_components):
    # Converting eigenvectors to real numbers if they are complex
    plt.arrow(0, 0, pca.eigenvectors[0, i].real, pca.eigenvectors[1, i].real,
              color='r', alpha=0.5, head_width=0.05, head_length=0.1)
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA of Annealing Dataset with Eigenvectors')
plt.colorbar(label='Class')
plt.show()

# Plot explained variance
plt.figure(figsize=(10, 6))
plt.bar(range(1, len(pca.explained_variance) + 1), pca.explained_variance)
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance by Principal Components')
plt.show()

"""#***Question 2***
Part 4
"""

from sklearn.manifold import TSNE

# t-SNE on original data
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)

plt.figure(figsize=(10, 8))
sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y, palette='deep', s=60)
plt.title('t-SNE of Original Data')
plt.show()

# t-SNE on reduced data
X_reduced_full_pca = pca.transform(X, n_components=20)
X_tsne_pca = tsne.fit_transform(X_reduced_full_pca.real)

plt.figure(figsize=(10, 8))
sns.scatterplot(x=X_tsne_pca[:, 0], y=X_tsne_pca[:, 1], hue=y, palette='deep')
plt.title('t-SNE on PCA-Transformed Data')
plt.show()

"""#***Question 3***
Part 1
"""

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score


# Separate features and target variable
X = df.drop('class', axis=1)
y = df['class'].values

# Normalize numerical inputs
scaler = StandardScaler()
numerical_columns = X.select_dtypes(include=[np.number]).columns
X[numerical_columns] = scaler.fit_transform(X[numerical_columns])

# Convert DataFrame to NumPy arrays after preprocessing
X = X.values
y = y.astype(int)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

n_components_range = [2, 5, 10, 20, 30]
train_accuracies = []
test_accuracies = []

for n in n_components_range:
    X_train_reduced = pca.transform(X_train, n)
    X_test_reduced = pca.transform(X_test, n)

    # Convert to real values
    X_train_reduced = X_train_reduced.real
    X_test_reduced = X_test_reduced.real

    clf = SVC(random_state=42)
    clf.fit(X_train_reduced, y_train)

    train_accuracies.append(accuracy_score(y_train, clf.predict(X_train_reduced)))
    test_accuracies.append(accuracy_score(y_test, clf.predict(X_test_reduced)))

plt.figure(figsize=(10, 6))
plt.plot(n_components_range, train_accuracies, label='Train Accuracy')
plt.plot(n_components_range, test_accuracies, label='Test Accuracy')
plt.xlabel('Number of Principal Components')
plt.ylabel('Accuracy')
plt.title('SVM Performance vs Number of Principal Components')
plt.legend()
plt.show()

"""#***Question 3***
Part 2
"""

# Train on reduced data
n_optimal = 20
X_train_reduced = pca.transform(X_train, n_optimal)
X_test_reduced = pca.transform(X_test, n_optimal)

# Convert to real values if necessary
X_train_reduced = X_train_reduced.real
X_test_reduced = X_test_reduced.real

clf_reduced = SVC(random_state=42)
clf_reduced.fit(X_train_reduced, y_train)
reduced_train_acc = accuracy_score(y_train, clf_reduced.predict(X_train_reduced))
reduced_test_acc = accuracy_score(y_test, clf_reduced.predict(X_test_reduced))

print("Actual Data - Train Accuracy:", actual_train_acc)
print("Actual Data - Test Accuracy:", actual_test_acc)
print("Reduced Data - Train Accuracy:", reduced_train_acc)
print("Reduced Data - Test Accuracy:", reduced_test_acc)